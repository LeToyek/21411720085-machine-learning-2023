{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLX06SQ3AX6hrqbBZd+rk0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeToyek/2141720085-machine-learning-2023/blob/main/praktikum_2(Tugas_Praktikum_Included).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkotOlFlcQjk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvkIrsAwcYNI",
        "outputId": "aeec9c21-fd3f-4fe3-fabf-e0a461133625"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAuxSwPrcegf",
        "outputId": "98d84b23-0dcc-42e9-80c9-4294c7a9f203"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCbYPb77chHv",
        "outputId": "cdb6f06b-ef64-47d4-d280-37fa81c0d867"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwYMgTSfck4C",
        "outputId": "ea71e038-3849-44f9-eb26-f211dc37540a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t40vr4RIcmwn",
        "outputId": "eb06b1b6-ae79-4a69-cf9d-a265baa907aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars=tf.keras.layers.StringLookup(vocabulary=list(vocab),mask_token=None)"
      ],
      "metadata": {
        "id": "JeVeEI9-crnz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2G7p3qc1jZ",
        "outputId": "6b4328c8-53c3-4e49-f221-ca44b797801b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "rMGIOt_Yc-ma"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwroFEHsdBLi",
        "outputId": "8ba493b4-6d65-4be3-9cf7-be5163d15b85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars,axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Labcn84dEFB",
        "outputId": "c3a93673-7334-43a6-e1e5-c878b6fb1e52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "tLmJ9E4NdGX7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dg1bYDxdMY4",
        "outputId": "b86aabb9-db50-4984-acca-61a188e79755"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "4hfxrh7qdULe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4i-YpEcdXWU",
        "outputId": "d2be2f38-2bbf-4a51-b471-9ec6fb76e452"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=100"
      ],
      "metadata": {
        "id": "Fpr8_8NOdbXZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmr9sO_jdfam",
        "outputId": "8f6bc8e5-035a-4dcb-a2f4-04095bf06c6c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XROHnUadhZ7",
        "outputId": "1209f886-1532-412a-f680-80dd366440b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        " input_text = sequence[:-1]\n",
        " target_text = sequence[1:]\n",
        " return input_text, target_text"
      ],
      "metadata": {
        "id": "himsXLofdl4_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoYQSViAdq-I",
        "outputId": "f5c292a0-20bd-4250-9be4-24d92f2952eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "NO7HI5A9dsqX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        " print(\"Input :\", text_from_ids(input_example).numpy())\n",
        " print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uu-6fcPdwQF",
        "outputId": "0c0a3e08-5684-4583-8236-e2310dc41df6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tyV6XBXd0HC",
        "outputId": "5442ebab-2faf-48af-da6b-3ff0fd41cea2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "rc7YUt62d2DN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "zbTfUdwTd39x"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "lTLP9KJMd5JM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeTYz66id6nD",
        "outputId": "80639d70-5eb8-4076-9237-93fc8b67842b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOvUWQnOd8SR",
        "outputId": "253d2ae4-f0eb-42cc-b711-6554ffdbaf9f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices=tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices=tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "metadata": {
        "id": "7914u1RpeHDV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFYEAQd3eN0z",
        "outputId": "c7814297-7369-4a52-bd5c-f3fe73ee2d09"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  7, 57, 33, 10, 60, 19,  7, 54, 22, 54, 49, 63, 50,  3, 38, 13,\n",
              "       49,  7,  7, 36, 61, 22, 63, 60, 47, 28,  0, 39, 16, 16, 43, 30, 39,\n",
              "       43,  0, 30, 39, 44, 39, 17, 12, 12, 11, 46, 49, 15,  8, 17, 38,  5,\n",
              "       44, 45, 52, 37, 45,  6, 15, 11, 10,  5, 36, 19, 35, 48, 63,  2, 41,\n",
              "       37, 36,  9, 14, 50, 44, 16,  0, 41, 23, 29, 62, 63, 19, 21, 31, 28,\n",
              "       31, 19,  1, 21, 58, 25,  4, 23,  5, 41, 11,  2, 10,  0, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGumQaw6ePdY",
        "outputId": "770055ef-251a-4a3b-a44c-75bcbbe01e45"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"\\nCome hither, England's hope.\\nIf secret powers\\nSuggest but truth to my divining thoughts,\\nThis prett\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\".,rT3uF,oIojxk!Y?j,,WvIxuhO[UNK]ZCCdQZd[UNK]QZeZD;;:gjB-DY&efmXf'B:3&WFVix bXW.AkeC[UNK]bJPwxFHRORF\\nHsL$J&b: 3[UNK]?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "Mntz8gyDeSzJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVPS14zPeWrv",
        "outputId": "d9daa3d1-41f1-4ea1-d342-1112877fc42a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1913357, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qp-CaAVeZJH",
        "outputId": "44d5999e-b8a2-45ab-ef02-b0dcab464986"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.11103"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "0nq91I33ebPS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "e9QGkUOPeeKw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=20"
      ],
      "metadata": {
        "id": "DGPpahK1ese7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV24XdtLe3KH",
        "outputId": "6f63ec36-0342-4956-8e7d-6a21d78c21c0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 20s 62ms/step - loss: 2.7300\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 14s 59ms/step - loss: 1.9984\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.7234\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.5617\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.4606\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3908\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 13s 65ms/step - loss: 1.3381\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2931\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2518\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2123\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1733\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1322\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.0893\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.0448\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9965\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9472\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8942\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8404\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.7876\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "sPyHe5wve6gL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "utM8RyeNjliN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gu6qkPajouI",
        "outputId": "219adceb-4b45-4b8f-bda9-5764c9cfce3f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Spakest thou off corrupt, and made\n",
            "More hate to England, all none falls and very\n",
            "abo.\n",
            "\n",
            "SEBASTIAN:\n",
            "You were known! what knock me with a tail upon these wounds;\n",
            "Therefore at that title shortly, as hun\n",
            "receives, and I'll be thy otherwisk.\n",
            "\n",
            "YORK:\n",
            "For what, I prophesy, if you crown to him.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "I vow'd it. How now, mantles, mistress commands as tender\n",
            "Than when he speak this business. Worse than wrong'd what satisficius\n",
            "Mayst thou behold'? on me; Buckingham,\n",
            "Bid him lead appeareth in this place:\n",
            "This may pay I no longer, that fur is there;\n",
            "her face impeach'd, and it becomes\n",
            "My talk appearable: and so attend on him.\n",
            "Making the downfall of his flood and his lab\n",
            "Timush'd, and bay stay and speak to thee rock.\n",
            "\n",
            "BAPOT:\n",
            "I do.\n",
            "\n",
            "HERMIONE:\n",
            "I pray you, sir, I shall resolve pude men'd\n",
            "until werk your case out, did read him dead;\n",
            "And who such wear oppress of the eight,\n",
            "We'll put you power to bring Edward\n",
            "Thou poin'd upon the shore, to her my heart\n",
            "To help him of your youth-be to dinner.\n",
            "\n",
            "ME \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4165587425231934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYfx5ZXFjpZt",
        "outputId": "e36c1a83-30d3-4765-bd45-5d8f22001463"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nGood my lord, be burnt in love and so less, your\\nhand, you shall bear a poor with't out: you shall\\nheed no more doth make the hardest thicker not\\nThe same pardon Rome.'\\n\\nAUFIDIUS:\\nThis your subject, I shall lie with thee\\nthat he rose that hust before him, 'fore these Capulet\\nUpon this coftar--heart'st and uncle, sir; and therein\\nIs my sweet villain; it is ten to order loves.\\n\\nNORTHUMBERLAND:\\nNorthumberland, Isabel, upon that God, I fear 'tis thus:\\nHold, then thou hast. O, my holy odd savour\\nIs do continue scorn his tatal golder incribents sound\\nTo Richmond, and if you had the words resorve\\nTo have that plantful pity make I must come all.\\n\\nESCALUS:\\nCommend me to it, let's see thee well:\\nThe absolute power, Edward will be farished:\\nI'll not be him, I hope to have a dogrement;\\nyea, and many fair prosperous enemies.\\n\\nFRIAR LAURENCE:\\nHark, Barrarity lends and flant Aufidius with\\nhim to the earth.\\n\\nSICINIUS:\\nThis is your son:\\nGood madam, sir, became comfort.\\nThe bound traitors! let him quit\"\n",
            " b\"ROMEO:\\n\\nJULIET:\\nSturn from me else. But who comes here?\\nDare need, I need not to require and honest:\\nI hear the Master your brother's son.\\n\\nJOHN OF GAUNT:\\nBut to us then to the wild revenge and do impare\\nWhich his grey audable days of hell all degrees\\nAnd made her bredict to pieces:\\nHis any that I did cower to have him\\nGod I be, that had shoke off their caps and\\nmetage, for it on my entrance:\\nBut I do wish me now, I want a knag of blood\\nWrit fetce of him, and duty to find I smiled\\nAnd that the englosed sullest poisons that\\nspurs his worself.\\n\\nLUCIO:\\nWell, fairly let's away. When, I beseech you\\nIn sign of death, do nighble stout,\\nThat wound they had carried Rome all that volust him at up,\\nMy soul to die that you might have made him\\nGo, and you still stay himself, for battle; thyself\\nAt Parish haste the enemies.\\n\\nMENENIUS:\\nWhen you will sit at onam?\\n\\nVOLUMNIA:\\nSo much i' the place is thus shall prove it off,\\nEnforced masterly, from which he divide up,\\nFor chorious presant grants with all near;\"\n",
            " b\"ROMEO:\\nSo thrive! I have with you,\\nSirs Claudio that my husband now, I have sent\\nbut that the field to hear it prined: come raise out\\nThe sun had done--but thine abroad?\\n\\nRATCLIFF:\\nMy ear, thou wrong'd,\\nThe clowness Camal of this night, and showest make\\nSchoold, feel; for, but say, justice news, and comes\\nAnd take her that have more discourse;\\nSince was my wit, that slew thy kingly thing;\\nYour PerditaMinf, that shall stood long ry\\nWarwick, in a match-born hoarding,\\nBlack, and bey, Grumio, poor soul,\\nIf this recompetted this slander's place, and they say.\\n\\nHENRY BOLINGBROKE:\\nRound wrong your guard keeps him to his son? Abaze fair so fair!\\n\\nESCALUS:\\nSay, yet remove thy basinardying belong and reverend\\nTo expend our hands; came then By King Edward's house,\\nIt was, less faith of name, and march him come to the maid;\\nNor I come buy and see where I am assured\\nAnd this being clean order Thereford that thy lord's faith.\\n\\nBRUTUS:\\nLet thy began nor\\nCame to your good lord chamberlain! Welcome! happy me\"\n",
            " b\"ROMEO:\\nShall I be great sorrow? why, that will not\\nposses: but they have shadoward minute ill sent up that\\ncontinue in pity of his face straight;\\nAnd pardon, fair, nor than. I am no\\nplucked us do; I'll go then instrument.\\n\\nBALTHASA:\\nHis name?\\n\\nCORIOLANUS:\\nI melt then did me thine. Has the eld has my mistress I\\nFor courtesy, and if I to remease him hence\\nAnd noble home to me: for my daughter's most\\nScratch my dishonour than the heart.\\nThat murder'd me for this old harse have.\\n\\nYORK:\\nO think I come!\\n\\nMENENIUS:\\nI am not for mocking: then am I may come\\nto prise at hand: speak thus.\\n\\nPETRUCHIO:\\nPardon me, look you had, may.\\n\\nDUKE VINCENTIO:\\nEither thou bid me swear I wonder? Then\\nTybalt look madam; who, I, poor soul,\\nI do this trial. Come, Clarence! I am\\nAgainst the God's hand with Farewell;\\nMy brother's love, daughmer; lady, and sounding--\\nDiddlf, urge Quoon! Nay, one worship's dear!\\nWho is it then? I resign my knees I was;\\nand on this royal presence I should knock you bat:\\nSirrah Grumio is pass\"\n",
            " b\"ROMEO:\\nShe may, I noted is tawn the sparklers slay to let:\\nThe new-daymen says your highness to my wife.\\n\\nBENVOLIO:\\nFetch on my soul, I see, sir.\\n\\nLUCIO:\\nDids, you have done--Padua to my friends will have it advanced:\\nThat by God's satisfy wish thine only differen\\nAnd darcing foe:\\nI say, this owe and full truer Earl of Marchant,\\nDear pronounced, often looks down and harbour\\nThe like antings.\\n\\nQUEEN MARGARET:\\nI tell thee, Licio, this bosom harmons, for methinks,\\nThou call'st my hand, and gentleman, sir Pifa' nod\\na bottle out of pilot; and the provost hate grant\\nThe fear-shippeet honeamest. Thou that, seeming how I came from health!\\nClaudio, while I pray you, farewell says, come, look and bear,\\nThan when I saw my inhant children left thee\\nThat by this leisurel and the main:\\nI, that look sorrow justice and sit claim\\ndo mistake, sleepy wandering, if always lost embrace!\\nThis country, toward his queen a love!\\nWhen julfure and not out this illd to have\\nSome course to with him talk always to yourse\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.955232858657837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwDz-FaNjrhI",
        "outputId": "a4fe6755-3b4c-4723-ea72-ee334e8d20df"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7bde802447c0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TKbUk2Tj4MM",
        "outputId": "3a0432ea-a1a8-4776-c0d3-1933fbd73bdc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Spakest thou to a festive tall yet upon\n",
            "on him that spoke 'gain that heaven 'tweer and false battle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas Praktikum"
      ],
      "metadata": {
        "id": "aWz8Lpv9ptbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "zTwdr7I0pxuy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "m_9D7Prwp5fe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "j0ZOHAjHp8OB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIZuZ-xFp-04",
        "outputId": "93c8b4a7-f223-41db-b6d2-8e56b44b197c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 16s 63ms/step - loss: 2.7341\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bde80244640>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoTQIvDIqA1H",
        "outputId": "43e644fc-460d-4e87-d335-2967758f992f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2088\n",
            "Epoch 1 Batch 50 Loss 2.0486\n",
            "Epoch 1 Batch 100 Loss 1.9678\n",
            "Epoch 1 Batch 150 Loss 1.8613\n",
            "\n",
            "Epoch 1 Loss: 1.9972\n",
            "Time taken for 1 epoch 14.02 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8173\n",
            "Epoch 2 Batch 50 Loss 1.7750\n",
            "Epoch 2 Batch 100 Loss 1.6788\n",
            "Epoch 2 Batch 150 Loss 1.6394\n",
            "\n",
            "Epoch 2 Loss: 1.7211\n",
            "Time taken for 1 epoch 12.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6365\n",
            "Epoch 3 Batch 50 Loss 1.5791\n",
            "Epoch 3 Batch 100 Loss 1.5535\n",
            "Epoch 3 Batch 150 Loss 1.4924\n",
            "\n",
            "Epoch 3 Loss: 1.5588\n",
            "Time taken for 1 epoch 12.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4974\n",
            "Epoch 4 Batch 50 Loss 1.4657\n",
            "Epoch 4 Batch 100 Loss 1.4798\n",
            "Epoch 4 Batch 150 Loss 1.4210\n",
            "\n",
            "Epoch 4 Loss: 1.4588\n",
            "Time taken for 1 epoch 12.05 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4119\n",
            "Epoch 5 Batch 50 Loss 1.4371\n",
            "Epoch 5 Batch 100 Loss 1.3551\n",
            "Epoch 5 Batch 150 Loss 1.3682\n",
            "\n",
            "Epoch 5 Loss: 1.3891\n",
            "Time taken for 1 epoch 11.85 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3343\n",
            "Epoch 6 Batch 50 Loss 1.2894\n",
            "Epoch 6 Batch 100 Loss 1.3663\n",
            "Epoch 6 Batch 150 Loss 1.3711\n",
            "\n",
            "Epoch 6 Loss: 1.3366\n",
            "Time taken for 1 epoch 11.67 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2701\n",
            "Epoch 7 Batch 50 Loss 1.3013\n",
            "Epoch 7 Batch 100 Loss 1.2994\n",
            "Epoch 7 Batch 150 Loss 1.3041\n",
            "\n",
            "Epoch 7 Loss: 1.2916\n",
            "Time taken for 1 epoch 11.88 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2568\n",
            "Epoch 8 Batch 50 Loss 1.2480\n",
            "Epoch 8 Batch 100 Loss 1.2471\n",
            "Epoch 8 Batch 150 Loss 1.2271\n",
            "\n",
            "Epoch 8 Loss: 1.2504\n",
            "Time taken for 1 epoch 12.09 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2089\n",
            "Epoch 9 Batch 50 Loss 1.2160\n",
            "Epoch 9 Batch 100 Loss 1.2336\n",
            "Epoch 9 Batch 150 Loss 1.2270\n",
            "\n",
            "Epoch 9 Loss: 1.2110\n",
            "Time taken for 1 epoch 12.08 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1312\n",
            "Epoch 10 Batch 50 Loss 1.1630\n",
            "Epoch 10 Batch 100 Loss 1.2020\n",
            "Epoch 10 Batch 150 Loss 1.2004\n",
            "\n",
            "Epoch 10 Loss: 1.1728\n",
            "Time taken for 1 epoch 12.25 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jawaban\n",
        "\n",
        "\n",
        "Pada praktikum 2, pendekatan pelatihan yang lebih umum dan sederhana dengan menggunakan metode model.fit dapat mengelola banyak aspek pelatihan, seperti perhitungan loss, gradien, dan pembaruan bobot model. Sementara kode pada tugas praktikum dibuat menjadi lebih detail sehingga parameter yang digunakan tidak hanya terbatas seperti pada function model.fit(). Perbedaan yang terlihat terdapat pada waktu pemrosesan per epoch. Kode pada tugas praktikum dapat memproses lebih cepat dari pada model.fit() dengan loss yang tidak jauh berbeda"
      ],
      "metadata": {
        "id": "4XruRZj8r0k4"
      }
    }
  ]
}